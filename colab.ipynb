{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit01d0416a27f84e10b5f2c7ca8689a6d4",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We need to tell Google Colab that we want the TF 2.0 version so the code can work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We also need to import our required libraries so we can use them in the next parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = tf.random.uniform(shape=[], minval=-5, maxval=5)\n",
    "b1 = tf.random.uniform(shape=[], minval=-5, maxval=5)\n",
    "c1 = tf.random.uniform(shape=[], minval=-5, maxval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "At this part we get some random values for a, b and c variables since our polynomial equation looks something like this: a*x^2 + b*x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = tf.random.uniform(shape=[], minval=-5, maxval=5)\n",
    "b2 = tf.random.uniform(shape=[], minval=-5, maxval=5)\n",
    "c2 = tf.random.uniform(shape=[], minval=-5, maxval=5)\n",
    "\n",
    "xs = tf.constant(range(0, 20), dtype=tf.float32)\n",
    "ys = tf.constant(tf.add(tf.add(tf.multiply(tf.pow(xs, 2), a2), tf.multiply(xs, b2)), c2), dtype=tf.float32)\n",
    "print(f\"Start values: \\nModel: ({a1})*x^2 + ({b1})*x + ({c1})\\nRandom Values: ({a2})*x^2 + ({b2})*x + ({c2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=tf.float32)\n",
    "# ys = tf.constant([45000.0, 50000.0, 60000.0, 80000.0, 110000.0, 150000.0, 200000.0, 300000.0, 500000.0, 1000000.0], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can either use the upper one or the bottom one while trying this out. The top one generates another random polynomial equation which we then try to find using our model. The bottom one on the other hand, is kind of well known dataset for trying out the polynomial regression. You may come accross that dataset whenever you are looking for polynomial regression tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ys, 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting the X and Y values before training our model so we can get an idea of how our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(predictions, labels):\n",
    "    return tf.reduce_mean(tf.square(predictions - labels))\n",
    "\n",
    "def stochastic_gradient_descent_optimizer(indexes, labels , predictions):\n",
    "    result = tf.reduce_mean(2 * indexes * (predictions - labels)).numpy()\n",
    "    print(f\"SGD --> Indexes: {indexes.numpy()} | Labels: {labels.numpy()} | Predictions: {predictions.numpy()} | Result: {result}\")\n",
    "    return result\n",
    "    \n",
    "def predict(indexes):\n",
    "    prediction = tf.add(tf.add(tf.multiply(tf.pow(indexes, 2), a1), tf.multiply(indexes, b1)), c1)\n",
    "    print(f\"Incoming: {indexes.numpy()} | Prediction: {prediction.numpy()}\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here, we declare our 3 main functions we need for our \"thing\" to become a bit of \"Machine Learning Model\". First one is Mean Squared Error. This function will tell a number based on how off our predictions are from the actual values. Next one is Stochastic Gradient Descent. This function will be acting as our optimizer in our model so we will be changing our a1, b1 and c1 values based on this value. And the final and the most important one(yes, all of them are very important but they won't make any sense without this one :D), our connection to model! With the prediction function, we can comminucate with our model and ask for predictions from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "SAMPLES = xs.shape[0]\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((xs , ys))\n",
    "dataset = dataset.repeat(EPOCHS).batch(BATCH_SIZE)\n",
    "iterator = dataset.__iter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "At this step, we are preparing our dataset to become iterable so we can train our model with the batches of data we just make here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(xs)\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    epoch_loss = list()\n",
    "    for Q in range(int(SAMPLES/BATCH_SIZE)):\n",
    "        x_batch, y_batch = iterator.get_next()\n",
    "        output = predict(x_batch)\n",
    "        loss_val = epoch_loss.append(mean_squared_error(y_batch , output).numpy())\n",
    "        deriv_val = stochastic_gradient_descent_optimizer(x_batch, y_batch , output)\n",
    "        # print(f\"deriv_val: {deriv_val}\")\n",
    "        a1 -= (LEARNING_RATE * deriv_val)\n",
    "        b1 -= (LEARNING_RATE * deriv_val)\n",
    "        c1 -= (LEARNING_RATE * deriv_val)\n",
    "    loss_val = np.array(epoch_loss).mean()\n",
    "    epochs_plot.append(i + 1)\n",
    "    loss_plot.append(loss_val)\n",
    "    print('Loss is {}'.format(loss_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And yet another very important step, training! At this step, we train our model using the functions we have defined a few steps ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs_plot, loss_plot) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here, we can see how our loss value lowered as we trained our model at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_points = list()\n",
    "for i in range(len(xs)):\n",
    "    polynomial_points.append(predict(xs[i]).numpy())\n",
    "plt.plot(xs, ys, 'bo', xs, polynomial_points, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And the final step! First, we just predict our values on the same X values as our dataset so we can match each other in the plot. After making the predictions, we just plot them both together and voila! We have just created and trained our own model for polynomial regression! You can get more information about this project (such as the math behind MSE and SGD) at my blog post which you can go with [this](https://blog.tekno.icu/2020/01/22/polynomial-regression-in-python-using-tensorflow-2-0/) link."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can also check how close our model became to the randomly generated quadratic equation if you have chosen to generate a random quadratic equation at the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"End values: \\nModel: ({a1})*x^2 + ({b1})*x + ({c1})\\nRandom Values: ({a2})*x^2 + ({b2})*x + ({c2})\")"
   ]
  }
 ]
}